---
title: "Lab9_Gradient Descent - Optimization"
author: "Sparsh Raj_19BPS1028"
date: "05/04/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Gradient Descent - Optimization
```{r}
rm(list=ls())
```

# Create a sequence of elements in a Vector to generate sequences when plotting the axes of figures or simulating data. 

```{r}
xs <- seq(0,4,len=20) 
xs
```


# Define the function we want to optimize
```{r}
f <-  function(x) {1.2 * (x-2)^2 + 3.2}
```

# Plot the function 
```{r}
plot(xs , f (xs), type="l",xlab="x",ylab=expression(1.2(x-2)^2 +3.2)) 
```

# calculate the gradient df/dx
```{r}
grad <- function(x){
  1.2*2*(x-2)
}
```


# df/dx = 2.4(x-2), if x = 2 then 2.4(2-2) = 0
# The actual solution we will approximate with gradient descent
# is  x = 2 as depicted in the plot below

#lines (c (2,2), c (3,8), col="red",lty=2)
#text (2.1,7, "Closedform solution",col="red",pos=4)

# gradient descent implementation
```{r}
x <- 0.1 # initialize the first guess for x-value
xtrace <- x # store x -values for graphing purposes (initial)
ftrace <- f(x) # store y-values (function evaluated at x) for graphing purposes (initial)
stepFactor <- 0.01 # learning rate 'alpha'
for (step in 1:5000) {
  x <- x - stepFactor*grad(x) # gradient descent update
  xtrace <- c(xtrace,x) # update for graph
  ftrace <- c(ftrace,f(x)) # update for graph
}
plot(xs , f (xs), type="l",xlab="x",ylab=expression(1.2(x-2)^2 +3.2))
lines ( xtrace , ftrace , type="b",col="blue") # type=b (both points & lines)
text (0.5,6, "Gradient Descent",col="red",pos= 4)
```


# print final value of x
```{r}
print(x) # x converges to 2.0
plot(xs , f (xs), type="l",xlab="x",ylab=expression(1.2(x-2)^2 +3.2))
text(2,4,"x=2",col="red",pos=1)
text(2,4,"(Global minimum)",col="red",pos=3)
```